### bgpls-broker

This project is a work in progress that exposes an API to the user what the network looks like using BGP Link State (BGP-LS). For more details, I recommend you take a look at these RFCs:

- https://tools.ietf.org/html/rfc8571
- https://tools.ietf.org/html/rfc7752

In a nutshell, ExaBGP exposes TCP/179 (BGP) to peer with your router and gather BGP-LS information. A local script runs under the API for ExaBGP which will read stdin and take this data, send a POST request to the bgplsapi container. This container is just a middle man and will pass every BGP UPDATE into a RabbitMQ Queue which workers will consume and perform the main functions (determines which type of BGP-LS update is it, updates/deletes MongoDB). The bgplsapi container then exposes an API to the user to gather all the details related to a specific ASN. Below is a diagram how this all works.

![bgplsapi Architecture](/img/bgpls-broker-architecture-igp-domains.jpg)

Sample configurations for exabgp and vendor configurations can be found at: [samples](../blob/master/samples/)

The application tries to maintain the state of the network based on UPDATEs received from the ExaBGP api. BGP-LS Nodes have a generated "node_id" which is globally unique as per ![BGP-LS extensions for Segment Routing](https://tools.ietf.org/html/draft-ietf-idr-bgpls-segment-routing-epe-19). The node_id is generated by using the ASN:Router-ID. These tuple values should be globally unique (otherwise your network is designed wrong). BGPLS-Links and BGPLS-Prefixes will have a relationship to a BGPLS-Node and therefore the API is able to pull all related BGP-LS Updates for a given ASN/Node.

Upon receiving an update to an existing entry in the database, it will update the entry. If a withdraw has been received then the workers will simply delete this based on a fairly strict search (eg. BPGLS-Node uses on the node_id but BGPLS-Link will check node_id, l3_routing_topology, interface-address and more...)

If a neighborship goes down between ExaBGP and the network, this is relayed to the API and will perform a cleanup for any related entries in the database (BGPLS-Node/Link/Prefixes) and will flush out the database. I have yet to still improve this feature/general database operations and cleanups since the database may not always be in sync with the network when the containers are restarted.

Currently, the API only exposes a few endpoints for the user. Here are the useful ones to gather some data if you are trying out this project and have successfully peered with the ExaBGP container from your network.

- /api/get_topology/<asn> - Returns the topology (more like a TED if TE functions are enabled) for given ASN
- /api/mongodb/get_collections - Returns the collections (eg. neighbor_state, bgpls_node, bgpls_links, bgpls_prefixes_v4 and bgpls_prefixes_v6)
- /api/mongodb/neighbor_state - Returns all entries in the neighbor_state collection
- /api/mongodb/bgpls_nodes - Returns all entries in the bgpls_nodes collection
- /api/mongodb/bgpls_links - Returns all entries in the bgpls_links collection
- /api/mongodb/bgpls_prefixes_v4 - Returns all entries in the bgpls_prefixes_v4 collection
- /api/mongodb/delete_collection/<collection> - Removes all entries in the collection specified.

You can find examples of what data is exposed from these databases in the samples folder (linked at the start of the readme). Please not that these samples may not be the most recent representation of data in the future if I forget to update documentation.

This project is not production ready and I would advise to run it in a lab if you want to play around with it.

## Low Level Design

How does this application work on a low level design?

You have a container which will run ExaBGP. Technically this container can be a standalone VM running ExaBGP or multiple containers spread across different virtual hosts in the network, the only requirement is that they can reach the BGPLSAPI container which is the frontend for taking the ExaBGP JSON messages and sending them to the RabbitMQ Queue. See below the current tested design and a custom redundant design (nothing stops the redundancy as far as the code goes, you'll probably just be performing actions against the database twice, if exabgp1 deletes an existing bgpls-link before exabgp2, exabgp2 will try to delete it again but it won't be found so some slight processing power is used at the expense of some redundancy...)

![bgplsapi Redundancy](/img/bgplsapi-redundancy.jpg)

The ExaBGP containers/vms run an attached process (python script) which will read the stdin, the magic happens on the encoder used in the process (`encoder json;`). This encoder will seralize data sent to ExaBGP and then the python script will be able to sent this data to the BGPLSAPI endpoint (specifically <server>/exabgp/).  Here is the ExaBGP process:

![bgplsapi ExaBGP Processing](/img/exabgp-process.jpg)

Once the BGPLSAPI endpoint (/exabgp/) receives this body, it will attempt to establish a connection to the RabbitMQ queue (default is `task_queue`) and will publish the JSON message received from the exabgpapi script and simply return `{"error": False}`. The BGPLSAPI exabgp endpoint doesn't care about returning any data to the exabgpapi request because it is just a middle man to send the request to RabbitMQ. I could actually send the json data directly to RabbitMQ from exabgpapi which may become a thing in the future.

There are workers which will subscribe (consume) to the RabbitMQ queue `task_queue` by default and will perform actions based on the type of UPDATE that exabgp had originally sent. The worker will pass the body data to a new thread which will then execute various functions based on the type of BGP message (eg. UPDATE for bgpls-node NLRI) and will perform actions against the MongoDB database, this includes inserting/updating/deleting/flushing. You can see the overall worker flow below:

![bgplsapi Worker Flow](/img/bgplsapi-worker-flow.jpg)

